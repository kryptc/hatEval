{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "DATA_PATH=\"../data/\"\n",
    "TRAIN_DATA = DATA_PATH + \"train_en.tsv\"\n",
    "TEST_DATA = DATA_PATH + \"dev_en.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(path):\n",
    "    data = []\n",
    "    with open(path,'r') as file:\n",
    "        data = [x for x in csv.reader(file, delimiter='\\t')]\n",
    "    return data\n",
    "\n",
    "def getTweets(raw):\n",
    "    data = [x[1] for x in raw if x[2] == '1']\n",
    "    return np.array(data)\n",
    "\n",
    "def getTarget(raw):\n",
    "    classes = [x[3] for x in raw if x[2] == '1']\n",
    "    return np.array(classes)\n",
    "\n",
    "def getAggression(raw):\n",
    "    classes = [x[4] for x in raw if x[2] == '1']\n",
    "    return classes\n",
    "\n",
    "def removePattern(tweet, pattern):\n",
    "    r = re.findall(pattern, tweet)\n",
    "    for x in r:\n",
    "        tweet = re.sub(x, '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess(data):\n",
    "    cleanData = []\n",
    "    for tweet in data:\n",
    "        tweet = removePattern(tweet, \"@[\\w]*\")\n",
    "        tweet = tweet.replace(\"#\", \"\") # Removing '#' from hashtags\n",
    "        tweet = re.sub(r'[^a-zA-Z]', \" \", tweet) # Removing punctuation and special characters\n",
    "        tweet = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',\"<URL>\", tweet)\n",
    "        tweet = re.sub(\" +\", \" \", tweet)\n",
    "        tweet = tweet.lower()\n",
    "        cleanData.append(tweet)\n",
    "    return cleanData\n",
    "\n",
    "def tokenize(text):\n",
    "    return TweetTokenizer.tokenize(text)\n",
    "\n",
    "def evaluate(target, predicted):\n",
    "    f1 = f1_score(target, predicted, average='weighted')\n",
    "    acc = accuracy_score(target, predicted)\n",
    "    rec = recall_score(target, predicted, average = 'macro')\n",
    "    print(\"F1 score:   \", f1)\n",
    "    print(\"Avg Recall: \", rec)    \n",
    "    print(\"Accuracy:   \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = set(stopwords.words(\"english\")) \n",
    "\n",
    "raw_train = readData(TRAIN_DATA) \n",
    "train_tweets = getTweets(raw_train)\n",
    "train_tweets = preprocess(train_tweets)\n",
    "Y_train_aggression = getAggression(raw_train)\n",
    "Y_train_target = getTarget(raw_train)\n",
    "\n",
    "raw_test = readData(TEST_DATA)\n",
    "test_tweets = getTweets(raw_test)\n",
    "test_tweets = preprocess(test_tweets)\n",
    "Y_test_aggression = getAggression(raw_test)\n",
    "Y_test_target = getTarget(raw_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tweets_count = len(train_tweets)\n",
    "# test_tweets_count = len(test_tweets)\n",
    "# # print(\"total number of train tweets: %s\", train_tweets_count)\n",
    "\n",
    "# train_tweets_with_aggression = np.sum(Y_train_aggression)\n",
    "# test_tweets_with_aggression = np.sum(Y_test_aggression)\n",
    "\n",
    "\n",
    "# train_tweets_without_aggression = train_tweets_count - train_tweets_with_aggression\n",
    "# test_tweets_without_aggression = test_tweets_count - test_tweets_with_aggression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings Word  Level and Char  Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(typ):\n",
    "    if typ == \"word\":\n",
    "        vectorizer = CountVectorizer(\n",
    "            analyzer = 'word',\n",
    "            lowercase = True,\n",
    "            ngram_range=(1, 1),\n",
    "            stop_words = en_stopwords)\n",
    "        vectorizer.fit(train_tweets)\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(\n",
    "            analyzer = 'char',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            ngram_range=(2, 6),\n",
    "            stop_words = en_stopwords)\n",
    "        vectorizer.fit(train_tweets)\n",
    "    return vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model(name):\n",
    "    if name == \"LR\":\n",
    "        classifier = LogisticRegression(C=0.1, solver='sag')\n",
    "    else:\n",
    "        classifier = SVC(C = 0.1)\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_type(name):\n",
    "    if name == \"TC\":\n",
    "        classifier.fit(train_features, Y_train_target)\n",
    "        y_predict = classifier.predict(test_features)\n",
    "        evaluate(Y_test_target, y_predict)\n",
    "    else:\n",
    "        classifier.fit(train_features, Y_train_aggression)\n",
    "        y_predict = classifier.predict(test_features)\n",
    "        evaluate(Y_test_aggression, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Target Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Level and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.8919342124852587\n",
      "Avg Recall:  0.8940112399016509\n",
      "Accuracy:    0.892271662763466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = embeddings(\"word\")\n",
    "train_features = vectorizer.transform(train_tweets)\n",
    "test_features = vectorizer.transform(test_tweets)\n",
    "classifier = model(\"LR\")\n",
    "task_type(\"TC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char Level and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.9063170196296517\n",
      "Avg Recall:  0.9069854232525465\n",
      "Accuracy:    0.9063231850117096\n"
     ]
    }
   ],
   "source": [
    "vectorizer = embeddings(\"char\")\n",
    "train_features = vectorizer.transform(train_tweets)\n",
    "test_features = vectorizer.transform(test_tweets)\n",
    "classifier = model(\"LR\")\n",
    "task_type(\"TC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Level and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/.local/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.31912076564199965\n",
      "Avg Recall:  0.5\n",
      "Accuracy:    0.48711943793911006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = embeddings(\"word\")\n",
    "train_features = vectorizer.transform(train_tweets)\n",
    "test_features = vectorizer.transform(test_tweets)\n",
    "classifier = model(\"SVM\")\n",
    "task_type(\"TC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char Level and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.31912076564199965\n",
      "Avg Recall:  0.5\n",
      "Accuracy:    0.48711943793911006\n"
     ]
    }
   ],
   "source": [
    "vectorizer = embeddings(\"char\")\n",
    "train_features = vectorizer.transform(train_tweets)\n",
    "test_features = vectorizer.transform(test_tweets)\n",
    "classifier = model(\"SVM\")\n",
    "task_type(\"TC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Aggression Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Level and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.6613086382884759\n",
      "Avg Recall:  0.660006154928339\n",
      "Accuracy:    0.6627634660421545\n"
     ]
    }
   ],
   "source": [
    "vectorizer = embeddings(\"word\")\n",
    "train_features = vectorizer.transform(train_tweets)\n",
    "test_features = vectorizer.transform(test_tweets)\n",
    "classifier = model(\"LR\")\n",
    "task_type(\"AD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char Level and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.6714586627966254\n",
      "Avg Recall:  0.6702277323485448\n",
      "Accuracy:    0.6721311475409836\n"
     ]
    }
   ],
   "source": [
    "vectorizer = embeddings(\"char\")\n",
    "train_features = vectorizer.transform(train_tweets)\n",
    "test_features = vectorizer.transform(test_tweets)\n",
    "classifier = model(\"LR\")\n",
    "task_type(\"AD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Level and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.3583426409655918\n",
      "Avg Recall:  0.5\n",
      "Accuracy:    0.522248243559719\n"
     ]
    }
   ],
   "source": [
    "vectorizer = embeddings(\"word\")\n",
    "train_features = vectorizer.transform(train_tweets)\n",
    "test_features = vectorizer.transform(test_tweets)\n",
    "classifier = model(\"SVM\")\n",
    "task_type(\"AD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char Level and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = embeddings(\"char\")\n",
    "train_features = vectorizer.transform(train_tweets)\n",
    "test_features = vectorizer.transform(test_tweets)\n",
    "classifier = model(\"SVM\")\n",
    "task_type(\"AD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
