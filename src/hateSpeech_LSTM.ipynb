{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/vaibhav/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/vaibhav/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/vaibhav/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/vaibhav/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/vaibhav/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/vaibhav/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import os\n",
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Embedding, GRU, Input, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "#     print(text)\n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    twtk = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    text = \" \".join([w for w in twtk.tokenize(text) if w != \"\" and w is not None])\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for cleaning\n",
    "def removeStopwords(tokens):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    stops.update(['.',',','\"',\"'\",'?',':',';','(',')','[',']','{','}'])\n",
    "    toks = [tok for tok in tokens if not tok in stops and len(tok) >= 3]\n",
    "    return toks\n",
    "\n",
    "def removeURL(text):\n",
    "    newText = re.sub('http\\\\S+', '', text, flags=re.MULTILINE)\n",
    "    return newText\n",
    "\n",
    "def removeNum(text):\n",
    "    newText = re.sub('\\\\d+', '', text)\n",
    "    return newText\n",
    "\n",
    "def removeHashtags(tokens):\n",
    "    toks = [ tok for tok in tokens if tok[0] != '#']\n",
    "#     if segment == True:\n",
    "#         segTool = Analyzer('en')\n",
    "#         for i, tag in enumerate(self.hashtags):\n",
    "#             text = tag.lstrip('#')\n",
    "#             segmented = segTool.segment(text)\n",
    "\n",
    "    return toks\n",
    "\n",
    "def stemTweet(tokens):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "    return stemmed_words\n",
    "\n",
    "def evaluate(target, predicted):\n",
    "    f1 = f1_score(target, predicted, average='weighted')\n",
    "    acc = accuracy_score(target, predicted)\n",
    "    rec = recall_score(target, predicted, average = 'macro')\n",
    "    print(\"F1 score:   \", f1)\n",
    "    print(\"Avg Recall: \", rec)    \n",
    "    print(\"Accuracy:   \", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTweet(tweet, remove_swords = True, remove_url = True, remove_hashtags = True, remove_num = True, stem_tweet = True):\n",
    "#     text = tweet.translate(string.punctuation)   -> to figure out what it does ?\n",
    "    \"\"\"\n",
    "        Tokenize the tweet text using TweetTokenizer.\n",
    "        set strip_handles = True to Twitter username handles.\n",
    "        set reduce_len = True to replace repeated character sequences of length 3 or greater with sequences of length 3.\n",
    "    \"\"\"\n",
    "    if remove_url:\n",
    "        tweet = removeURL(tweet)\n",
    "    twtk = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    tokens = [w.lower() for w in twtk.tokenize(tweet) if w != \"\" and w is not None]\n",
    "    if remove_hashtags:\n",
    "        tokens = removeHashtags(tokens)\n",
    "    if remove_swords:\n",
    "        tokens = removeStopwords(tokens)\n",
    "    if stem_tweet:\n",
    "        tokens = stemTweet(tokens)\n",
    "    text = \" \".join(tokens)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/train_en.tsv',delimiter='\\t',encoding='utf-8')\n",
    "cleaned = train_data['text'][:5].map(lambda x: processTweet(x))\n",
    "# for i,t in enumerate(cleaned):\n",
    "#     print (i,train_data['text'][i]\n",
    "#     print \n",
    "#     print t\n",
    "#     print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vaibhav/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vaibhav/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vaibhav/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/.local/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(10687, 128, input_length=140, embeddings_regularizer=<keras.reg...)`\n",
      "/home/vaibhav/.local/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `GRU` call to the Keras 2 API: `GRU(128, return_sequences=False, kernel_regularizer=<keras.reg..., bias_regularizer=<keras.reg..., recurrent_regularizer=<keras.reg...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vaibhav/.local/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vaibhav/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/.local/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2, activation=\"softmax\", kernel_regularizer=<keras.reg...)`\n",
      "/home/vaibhav/.local/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vaibhav/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/vaibhav/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 8100 samples, validate on 900 samples\n",
      "Epoch 1/10\n",
      "8100/8100 [==============================] - 55s 7ms/step - loss: 0.7361 - acc: 0.7107 - val_loss: 1.2540 - val_acc: 0.4267\n",
      "Epoch 2/10\n",
      "8100/8100 [==============================] - 49s 6ms/step - loss: 0.5554 - acc: 0.7632 - val_loss: 0.8940 - val_acc: 0.5856\n",
      "Epoch 3/10\n",
      "8100/8100 [==============================] - 48s 6ms/step - loss: 0.5438 - acc: 0.7681 - val_loss: 0.9117 - val_acc: 0.5867\n",
      "Epoch 4/10\n",
      "8100/8100 [==============================] - 48s 6ms/step - loss: 0.5351 - acc: 0.7728 - val_loss: 0.8409 - val_acc: 0.5867\n",
      "Epoch 5/10\n",
      "8100/8100 [==============================] - 48s 6ms/step - loss: 0.5268 - acc: 0.7730 - val_loss: 1.1069 - val_acc: 0.5467\n",
      "Epoch 6/10\n",
      "8100/8100 [==============================] - 48s 6ms/step - loss: 0.5222 - acc: 0.7788 - val_loss: 0.8753 - val_acc: 0.5911\n",
      "Epoch 7/10\n",
      "8100/8100 [==============================] - 48s 6ms/step - loss: 0.5168 - acc: 0.7805 - val_loss: 0.8882 - val_acc: 0.5900\n",
      "Epoch 8/10\n",
      "8100/8100 [==============================] - 48s 6ms/step - loss: 0.5121 - acc: 0.7858 - val_loss: 0.9393 - val_acc: 0.5656\n",
      "Epoch 9/10\n",
      "8100/8100 [==============================] - 48s 6ms/step - loss: 0.5092 - acc: 0.7846 - val_loss: 0.9796 - val_acc: 0.5622\n",
      "Epoch 10/10\n",
      "8100/8100 [==============================] - 48s 6ms/step - loss: 0.5097 - acc: 0.7886 - val_loss: 1.0641 - val_acc: 0.5433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f83cb4922b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweets = train_data['text']\n",
    "maxlen = 140\n",
    "train_data['text'] = train_data['text'].map(lambda x: processTweet(x))\n",
    "vocabulary_size = 30000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "X_train = tokenizer.texts_to_sequences(train_data['text'])\n",
    "# print(sequences)\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "labels = train_data['HS']\n",
    "Y_train = np_utils.to_categorical(labels, len(set(labels)))\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "l2_coef = 0.001\n",
    "tweet = Input(shape=(maxlen,), dtype='int32')\n",
    "x = Embedding(V, 128, input_length=maxlen, W_regularizer=l2(l=l2_coef))(tweet)\n",
    "x = Bidirectional(layer=GRU(128, return_sequences=False, \n",
    "                            W_regularizer=l2(l=l2_coef),\n",
    "                            b_regularizer=l2(l=l2_coef),\n",
    "                            U_regularizer=l2(l=l2_coef)),\n",
    "                  merge_mode='sum')(x)\n",
    "x = Dense(len(set(labels)), W_regularizer=l2(l=l2_coef), activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(input=tweet, output=x)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='RMSprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/dev_en.tsv',delimiter='\\t',encoding='utf-8')\n",
    "cleaned_test = test_data['text'][:5].map(lambda x: processTweet(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 140\n",
    "test_data['text'] = test_data['text'].map(lambda x: processTweet(x))\n",
    "vocabulary_size = 30000\n",
    "# tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "# tokenizer.fit_on_texts(train_data['text'])\n",
    "X_test = tokenizer.texts_to_sequences(test_data['text'])\n",
    "# print(sequences)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "labels = test_data['HS']\n",
    "Y_test = np_utils.to_categorical(labels, len(set(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6518227  0.3481773 ]\n",
      " [0.97245544 0.02754457]\n",
      " [0.23733237 0.76266766]\n",
      " ...\n",
      " [0.00919168 0.9908083 ]\n",
      " [0.02382755 0.97617245]\n",
      " [0.7291961  0.27080396]]\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict(X_test)\n",
    "print(y_predict)\n",
    "y_predicted = []\n",
    "for i in y_predict:\n",
    "    if i[0] < i[1]:\n",
    "        y_predicted.append(1)\n",
    "    else:\n",
    "        y_predicted.append(0)\n",
    "        \n",
    "y_test = []\n",
    "for i in Y_test:\n",
    "    if i[0] == 1:\n",
    "        y_test.append(0)\n",
    "    else:\n",
    "        y_test.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.6476282377471362\n",
      "Avg Recall:  0.6370064290414474\n",
      "Accuracy:    0.65\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test, y_predicted)\n",
    "# print(y_predicted[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
