{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/vaibhav/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/vaibhav/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/vaibhav/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/vaibhav/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/vaibhav/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/vaibhav/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import py_crepe\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(tokens):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    stops.update(['.',',','\"',\"'\",'?',':',';','(',')','[',']','{','}'])\n",
    "    toks = [tok for tok in tokens if not tok in stops and len(tok) >= 3]\n",
    "    return toks\n",
    "\n",
    "def removeURL(text):\n",
    "    newText = re.sub('http\\\\S+', '', text, flags=re.MULTILINE)\n",
    "    return newText\n",
    "\n",
    "def removeNum(text):\n",
    "    newText = re.sub('\\\\d+', '', text)\n",
    "    return newText\n",
    "\n",
    "def removeHashtags(tokens):\n",
    "    toks = [ tok for tok in tokens if tok[0] != '#']\n",
    "#     if segment == True:\n",
    "#         segTool = Analyzer('en')\n",
    "#         for i, tag in enumerate(self.hashtags):\n",
    "#             text = tag.lstrip('#')\n",
    "#             segmented = segTool.segment(text)\n",
    "\n",
    "    return toks\n",
    "\n",
    "def stemTweet(tokens):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "    return stemmed_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTweet(tweet, remove_swords = True, remove_url = True, remove_hashtags = True, remove_num = True, stem_tweet = True):\n",
    "#     text = tweet.translate(string.punctuation)   -> to figure out what it does ?\n",
    "    \"\"\"\n",
    "        Tokenize the tweet text using TweetTokenizer.\n",
    "        set strip_handles = True to Twitter username handles.\n",
    "        set reduce_len = True to replace repeated character sequences of length 3 or greater with sequences of length 3.\n",
    "    \"\"\"\n",
    "    if remove_url:\n",
    "        tweet = removeURL(tweet)\n",
    "    twtk = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    if remove_num:\n",
    "        tweet = removeNum(tweet)\n",
    "    tokens = [w.lower() for w in twtk.tokenize(tweet) if w != \"\" and w is not None]\n",
    "    if remove_hashtags:\n",
    "        tokens = removeHashtags(tokens)\n",
    "    if remove_swords:\n",
    "        tokens = removeStopwords(tokens)\n",
    "    if stem_tweet:\n",
    "        tokens = stemTweet(tokens)\n",
    "    text = \" \".join(tokens)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ag_data():\n",
    "    train = pd.read_csv('../data/train_en.tsv', delimiter='\\t', encoding='utf-8')\n",
    "    train = train.dropna()\n",
    "    train = train.loc[train['HS'] == 1]\n",
    "\n",
    "    x_train = train['text'].map(lambda x: processTweet(x, remove_swords = False, remove_url = True, \n",
    "                                remove_hashtags = False, remove_num = True, stem_tweet = False))\n",
    "    x_train = np.array(x_train)\n",
    "\n",
    "    y_train = train['TR']\n",
    "    y_train = to_categorical(y_train)\n",
    "\n",
    "    test = pd.read_csv('../data/dev_en.tsv', delimiter='\\t', encoding='utf-8')\n",
    "    test = test.loc[test['HS'] == 1]\n",
    "    x_test = test['text'].map(lambda x: processTweet(x, remove_swords = False, remove_url = True, \n",
    "                                remove_hashtags = False, remove_num = True, stem_tweet = False))\n",
    "    x_test = np.array(x_test)\n",
    "\n",
    "    y_test = test['TR']\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(x, maxlen, vocab):\n",
    "    # Iterate over the loaded data and create a matrix of size (len(x), maxlen)\n",
    "    # Each character is encoded into a one-hot array later at the lambda layer.\n",
    "    # Chars not in the vocab are encoded as -1, into an all zero vector.\n",
    "\n",
    "    input_data = np.zeros((len(x), maxlen), dtype=np.int)\n",
    "    for dix, sent in enumerate(x):\n",
    "        counter = 0\n",
    "        for c in sent:\n",
    "            if counter >= maxlen:\n",
    "                pass\n",
    "            else:\n",
    "                ix = vocab.get(c, -1)  # get index from vocab dictionary, if not in vocab, return -1\n",
    "                input_data[dix, counter] = ix\n",
    "                counter += 1\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_set():\n",
    "    # This alphabet is 69 chars vs. 70 reported in the paper since they include two\n",
    "    # '-' characters. See https://github.com/zhangxiangxiao/Crepe#issues.\n",
    "\n",
    "    alphabet = set(list(string.ascii_lowercase) + list(string.digits) +\n",
    "                   list(string.punctuation) + ['\\n'])\n",
    "    vocab_size = len(alphabet)\n",
    "    vocab = {}\n",
    "    reverse_vocab = {}\n",
    "    for ix, t in enumerate(alphabet):\n",
    "        vocab[t] = ix\n",
    "        reverse_vocab[ix] = t\n",
    "\n",
    "    return vocab, reverse_vocab, vocab_size, alphabet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_ag_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vaibhav/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 512, 69)           0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv1D)               (None, 506, 256)          123904    \n",
      "_________________________________________________________________\n",
      "MaxPool1 (MaxPooling1D)      (None, 168, 256)          0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv1D)               (None, 162, 256)          459008    \n",
      "_________________________________________________________________\n",
      "MaxPool2 (MaxPooling1D)      (None, 54, 256)           0         \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv1D)               (None, 52, 256)           196864    \n",
      "_________________________________________________________________\n",
      "Conv4 (Conv1D)               (None, 50, 256)           196864    \n",
      "_________________________________________________________________\n",
      "Conv5 (Conv1D)               (None, 48, 256)           196864    \n",
      "_________________________________________________________________\n",
      "Conv6 (Conv1D)               (None, 46, 256)           196864    \n",
      "_________________________________________________________________\n",
      "MaxPool3 (MaxPooling1D)      (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3840)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              3933184   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 6,355,202\n",
      "Trainable params: 6,355,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)  # for reproducibility\n",
    "\n",
    "# set parameters:\n",
    "\n",
    "subset = None\n",
    "\n",
    "# Whether to save model parameters\n",
    "save = False\n",
    "model_name_path = 'params/crepe_model.json'\n",
    "model_weights_path = 'params/crepe_model_weights.h5'\n",
    "\n",
    "# Maximum length. Longer gets chopped. Shorter gets padded.\n",
    "maxlen = 512\n",
    "\n",
    "# Model params\n",
    "# Filters for conv layers\n",
    "nb_filter = 256\n",
    "# Number of units in the dense layer\n",
    "dense_outputs = 1024\n",
    "# Conv layer kernel size\n",
    "filter_kernels = [7, 7, 3, 3, 3, 3]\n",
    "# Number of units in the final output layer. Number of classes.\n",
    "cat_output = 2\n",
    "\n",
    "# Compile/fit params\n",
    "batch_size = 80\n",
    "nb_epoch = 20\n",
    "\n",
    "vocab, reverse_vocab, vocab_size, alphabet = create_vocab_set()\n",
    "model = py_crepe.create_model(filter_kernels, dense_outputs, maxlen, vocab_size,\n",
    "                              nb_filter, cat_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = encode_data(x_train, maxlen, vocab)\n",
    "x_test = encode_data(x_test, maxlen, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"./weights-best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "            save_best_only=True, mode='max', save_weights_only=True)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3783/3783 [==============================] - 65s 17ms/step - loss: 0.6454 - acc: 0.6564\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/.local/lib/python3.6/site-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3783/3783 [==============================] - 72s 19ms/step - loss: 0.5511 - acc: 0.7082\n",
      "Epoch 3/20\n",
      "3783/3783 [==============================] - 70s 19ms/step - loss: 0.4617 - acc: 0.7920\n",
      "Epoch 4/20\n",
      "3783/3783 [==============================] - 87s 23ms/step - loss: 0.4100 - acc: 0.8292\n",
      "Epoch 5/20\n",
      "3783/3783 [==============================] - 86s 23ms/step - loss: 0.3550 - acc: 0.8562\n",
      "Epoch 6/20\n",
      "3783/3783 [==============================] - 84s 22ms/step - loss: 0.2903 - acc: 0.8898\n",
      "Epoch 7/20\n",
      "3783/3783 [==============================] - 83s 22ms/step - loss: 0.2128 - acc: 0.9247\n",
      "Epoch 8/20\n",
      "3783/3783 [==============================] - 71s 19ms/step - loss: 0.2346 - acc: 0.9165\n",
      "Epoch 9/20\n",
      "3783/3783 [==============================] - 67s 18ms/step - loss: 0.1554 - acc: 0.9487\n",
      "Epoch 10/20\n",
      "3783/3783 [==============================] - 89s 24ms/step - loss: 0.1047 - acc: 0.9646\n",
      "Epoch 11/20\n",
      "3783/3783 [==============================] - 83s 22ms/step - loss: 0.1172 - acc: 0.9619\n",
      "Epoch 12/20\n",
      "3783/3783 [==============================] - 83s 22ms/step - loss: 0.0584 - acc: 0.9833\n",
      "Epoch 13/20\n",
      "3783/3783 [==============================] - 83s 22ms/step - loss: 0.0607 - acc: 0.9839\n",
      "Epoch 14/20\n",
      "3783/3783 [==============================] - 85s 23ms/step - loss: 0.0544 - acc: 0.9836\n",
      "Epoch 15/20\n",
      "3783/3783 [==============================] - 83s 22ms/step - loss: 0.0761 - acc: 0.9757\n",
      "Epoch 16/20\n",
      "3783/3783 [==============================] - 83s 22ms/step - loss: 0.0812 - acc: 0.9752\n",
      "Epoch 17/20\n",
      "3783/3783 [==============================] - 83s 22ms/step - loss: 0.0359 - acc: 0.9897\n",
      "Epoch 18/20\n",
      "3783/3783 [==============================] - 84s 22ms/step - loss: 0.0315 - acc: 0.9923\n",
      "Epoch 19/20\n",
      "3783/3783 [==============================] - 85s 22ms/step - loss: 0.0484 - acc: 0.9860\n",
      "Epoch 20/20\n",
      "3783/3783 [==============================] - 83s 22ms/step - loss: 0.0391 - acc: 0.9884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f10726f5ac8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=nb_epoch, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision\t [0.68275862 0.9270073 ]\n",
      "Recall   \t [0.95192308 0.57990868]\n",
      "F1-Score \t [0.79518072 0.71348315]\n",
      "ROC-AUC  \t 0.7659158763610818\n"
     ]
    }
   ],
   "source": [
    "# model.load_weights(filepath)\n",
    "\n",
    "y_predict = model.predict(x_test, batch_size=None, steps=None)\n",
    "\n",
    "y_predict = np.argmax(y_predict, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"Precision\\t\", precision_score(y_test, y_predict, average=None))\n",
    "print(\"Recall   \\t\", recall_score(y_test, y_predict, average=None))\n",
    "print(\"F1-Score \\t\", f1_score(y_test, y_predict, average=None))\n",
    "print(\"ROC-AUC  \\t\", roc_auc_score(y_test, y_predict, average=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
